"""
ğŸ“ scripts/run_migrate_to_db.py
==================================
CSV ë°ì´í„° â†’ MySQL ë§ˆì´ê·¸ë ˆì´ì…˜.

ê¸°ì¡´ì— ìˆ˜ì§‘í•œ stores_raw.csv, hdong_codes_*.csvë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤.

ì‹¤í–‰:
  python scripts/run_migrate_to_db.py              # ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜
  python scripts/run_migrate_to_db.py --stores      # ìƒê°€ ë°ì´í„°ë§Œ
  python scripts/run_migrate_to_db.py --regions     # í–‰ì •ë™ ì½”ë“œë§Œ

í•„ìš”:
  docker-compose up -d   (MySQL ë¨¼ì € ì‹¤í–‰)
"""

import sys
import argparse
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv
load_dotenv()

import pandas as pd

from config.settings import get_settings
from src.database.repository import StoreRepository, RegionRepository
from src.utils.logger import setup_logging, get_logger
from src.utils.timer import timer

logger = get_logger(__name__)


@timer("CSV â†’ DB ë§ˆì´ê·¸ë ˆì´ì…˜")
def migrate_stores(data_path: str):
    """stores_raw.csv â†’ stores í…Œì´ë¸”"""
    path = Path(data_path)
    if not path.exists():
        logger.error("íŒŒì¼ ì—†ìŒ: %s (make collect ë¨¼ì € ì‹¤í–‰)", data_path)
        return

    logger.info("ğŸ“¥ CSV ë¡œë“œ: %s", data_path)
    df = pd.read_csv(data_path, dtype=str, low_memory=False)
    logger.info("  %dí–‰ Ã— %dì—´", *df.shape)

    repo = StoreRepository()
    saved = repo.upsert_stores(df)
    logger.info("âœ… stores í…Œì´ë¸” ì €ì¥: %dê±´", saved)

    # ì €ì¥ í™•ì¸
    total = repo.get_store_count()
    logger.info("  DB ì „ì²´ ìƒê°€ ìˆ˜: %dê±´", total)


@timer("í–‰ì •ë™ ì½”ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜")
def migrate_regions():
    """hdong_codes_*.csv â†’ region_codes í…Œì´ë¸”"""
    settings = get_settings()
    region_dir = Path(settings.DATA_RAW).parent / "00_region_codes"

    csv_files = sorted(region_dir.glob("hdong_codes_*.csv"))
    if not csv_files:
        logger.warning("í–‰ì •ë™ ì½”ë“œ CSV ì—†ìŒ: %s", region_dir)
        return

    repo = RegionRepository()

    for csv_path in csv_files:
        logger.info("ğŸ“¥ ë¡œë“œ: %s", csv_path.name)
        df = pd.read_csv(csv_path, dtype=str)

        # ì‹œë„/ì‹œêµ°êµ¬ ë ˆë²¨ ì œê±°
        if "region_cd_8" in df.columns:
            before = len(df)
            df = df[~df["region_cd_8"].str.endswith("0000")]
            logger.info("  í•„í„°: %d â†’ %dê±´ (ì‹œë„/ì‹œêµ°êµ¬ ì œê±°)", before, len(df))

        saved = repo.upsert_regions(df)
        logger.info("  ì €ì¥: %dê±´", saved)


@timer("ì •ì œ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜")
def migrate_cleaned(data_path: str, pipeline_run_id: str):
    """cleaned.csv â†’ cleaned_stores í…Œì´ë¸”"""
    path = Path(data_path)
    if not path.exists():
        logger.info("ì •ì œ ë°ì´í„° ì—†ìŒ (ê±´ë„ˆëœ€): %s", data_path)
        return

    logger.info("CSV ë¡œë“œ: %s", data_path)
    df = pd.read_csv(data_path, low_memory=False)
    logger.info("  %dí–‰ Ã— %dì—´", *df.shape)

    from src.database.repository import CleanedStoreRepository
    repo = CleanedStoreRepository()
    saved = repo.save_cleaned(df, pipeline_run_id)
    logger.info("cleaned_stores ì €ì¥: %dê±´", saved)


@timer("ë¼ë²¨ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜")
def migrate_labeled(data_path: str, pipeline_run_id: str):
    """labeled.csv â†’ labeled_stores í…Œì´ë¸”"""
    path = Path(data_path)
    if not path.exists():
        logger.info("ë¼ë²¨ ë°ì´í„° ì—†ìŒ (ê±´ë„ˆëœ€): %s", data_path)
        return

    logger.info("CSV ë¡œë“œ: %s", data_path)
    df = pd.read_csv(data_path, low_memory=False)
    logger.info("  %dí–‰ Ã— %dì—´", *df.shape)

    from src.database.repository import LabeledStoreRepository
    repo = LabeledStoreRepository()
    saved = repo.save_labeled(df, pipeline_run_id)
    logger.info("labeled_stores ì €ì¥: %dê±´", saved)


def main():
    parser = argparse.ArgumentParser(description="CSV â†’ DB ë§ˆì´ê·¸ë ˆì´ì…˜")
    parser.add_argument("--stores", action="store_true", help="ìƒê°€ ë°ì´í„°ë§Œ")
    parser.add_argument("--regions", action="store_true", help="í–‰ì •ë™ ì½”ë“œë§Œ")
    parser.add_argument("--cleaned", action="store_true", help="ì •ì œ ë°ì´í„°ë§Œ")
    parser.add_argument("--labeled", action="store_true", help="ë¼ë²¨ ë°ì´í„°ë§Œ")
    parser.add_argument("--all", action="store_true", help="ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜")
    args = parser.parse_args()

    setup_logging()
    settings = get_settings()

    # ì•„ë¬´ê²ƒë„ ì§€ì • ì•ˆ í•˜ë©´ ì „ì²´
    no_flags = not any([args.stores, args.regions, args.cleaned, args.labeled])
    do_all = args.all or no_flags

    logger.info("â”â”â” CSV â†’ MySQL ë§ˆì´ê·¸ë ˆì´ì…˜ â”â”â”")
    logger.info("DB: %s", settings.DATABASE_URL.split("@")[-1])

    if do_all or args.regions:
        migrate_regions()

    if do_all or args.stores:
        migrate_stores(f"{settings.DATA_RAW}/stores_raw.csv")

    # ì¤‘ê°„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ (ê³µí†µ pipeline_run_id ì‚¬ìš©)
    import uuid
    pipeline_run_id = str(uuid.uuid4())

    if do_all or args.cleaned:
        migrate_cleaned(f"{settings.DATA_INTERIM}/cleaned.csv", pipeline_run_id)

    if do_all or args.labeled:
        migrate_labeled(f"{settings.DATA_PROCESSED}/labeled.csv", pipeline_run_id)

    logger.info("")
    logger.info("ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")


if __name__ == "__main__":
    main()