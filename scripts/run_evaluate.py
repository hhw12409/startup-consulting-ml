"""
ğŸ“ scripts/run_evaluate.py
============================
ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€í•©ë‹ˆë‹¤.

ì‹¤í–‰:
  python scripts/run_evaluate.py                     # ëª¨ë“  ëª¨ë¸ í‰ê°€ (ìë™ íƒìƒ‰)
  python scripts/run_evaluate.py --model xgboost     # XGBoostë§Œ
  python scripts/run_evaluate.py --model neural_net  # PyTorchë§Œ
  python scripts/run_evaluate.py --model all         # ì „ì²´ + ë¹„êµ ë¦¬í¬íŠ¸
"""

import os
# XGBoost + PyTorch ë™ì‹œ ì‚¬ìš© ì‹œ OpenMP ìŠ¤ë ˆë”© ì¶©ëŒ ë°©ì§€
os.environ["OMP_NUM_THREADS"] = "1"

import sys
import argparse
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from config.settings import get_settings
from src.features.store import FeatureStore
from src.evaluation.metrics import evaluate_model
from src.evaluation.reporter import EvaluationReporter
from src.utils.logger import setup_logging, get_logger

logger = get_logger(__name__)


def load_model(model_type: str, model_path: str):
    """ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ í´ë˜ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    if model_type == "xgboost":
        from src.models.xgboost_model import XGBoostModel
        model = XGBoostModel()
    elif model_type == "neural_net":
        from src.models.neural_net import NeuralNetModel
        model = NeuralNetModel()
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_type}")

    model.load(model_path)
    return model


def find_saved_models(registry_dir: str) -> list[tuple[str, str]]:
    """
    ì €ì¥ëœ ëª¨ë¸ì„ ìë™ íƒìƒ‰í•©ë‹ˆë‹¤.

    Returns:
        [(model_type, model_path), ...]
    """
    registry = Path(registry_dir)
    found = []

    # XGBoost (best_model.pkl ë˜ëŠ” xgboost_model.pkl)
    for name in ["best_model", "xgboost_model"]:
        if (registry / f"{name}.pkl").exists():
            found.append(("xgboost", str(registry / name)))
            break

    # PyTorch (neural_net_model.pt ë˜ëŠ” best_model.pt)
    for name in ["neural_net_model", "best_model"]:
        if (registry / f"{name}.pt").exists():
            found.append(("neural_net", str(registry / name)))
            break

    return found


def main():
    parser = argparse.ArgumentParser(description="ëª¨ë¸ í‰ê°€")
    parser.add_argument(
        "--model",
        choices=["xgboost", "neural_net", "all"],
        default="all",
        help="í‰ê°€í•  ëª¨ë¸ (ê¸°ë³¸: all)",
    )
    args = parser.parse_args()

    setup_logging()
    s = get_settings()

    # â”€â”€ 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ â”€â”€
    store = FeatureStore()
    try:
        X_test, y_test = store.load("test")
    except FileNotFoundError:
        logger.error("í…ŒìŠ¤íŠ¸ ë°ì´í„° ì—†ìŒ (make train ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”)")
        return

    logger.info("í…ŒìŠ¤íŠ¸ ë°ì´í„°: %dí–‰ Ã— %dì—´", *X_test.shape)

    # â”€â”€ 2. ëª¨ë¸ ë¡œë“œ & í‰ê°€ â”€â”€
    all_metrics = {}
    reporter = EvaluationReporter()

    if args.model == "all":
        # ì €ì¥ëœ ëª¨ë¸ ìë™ íƒìƒ‰
        saved = find_saved_models(s.MODEL_REGISTRY)
        if not saved:
            logger.error("ì €ì¥ëœ ëª¨ë¸ ì—†ìŒ: %s (make train ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”)", s.MODEL_REGISTRY)
            return
        logger.info("ë°œê²¬ëœ ëª¨ë¸: %s", [m[0] for m in saved])
    else:
        # íŠ¹ì • ëª¨ë¸ë§Œ
        model_path = f"{s.MODEL_REGISTRY}/best_model"
        saved = [(args.model, model_path)]

    for model_type, model_path in saved:
        logger.info("")
        logger.info("â”" * 50)
        logger.info("ğŸ“Š %s í‰ê°€ ì‹œì‘", model_type.upper())
        logger.info("â”" * 50)

        try:
            model = load_model(model_type, model_path)
            metrics = evaluate_model(model, X_test, y_test)
            all_metrics[model_type] = metrics

            # ê°œë³„ ë¦¬í¬íŠ¸ ì €ì¥
            reporter.generate(
                metrics, model.get_info(),
                save_path=f"{s.LOG_DIR}/eval_{model_type}.json",
            )
        except Exception as e:
            logger.error("%s í‰ê°€ ì‹¤íŒ¨: %s", model_type, e)

    # â”€â”€ 3. ë¹„êµ ë¦¬í¬íŠ¸ (2ê°œ ì´ìƒ ëª¨ë¸ì¼ ë•Œ) â”€â”€
    if len(all_metrics) >= 2:
        logger.info("")
        logger.info("â”" * 50)
        logger.info("ğŸ“Š ëª¨ë¸ ë¹„êµ")
        logger.info("â”" * 50)

        # ê³µí†µ ë©”íŠ¸ë¦­ìœ¼ë¡œ ë¹„êµ
        all_keys = set()
        for m in all_metrics.values():
            all_keys.update(m.keys())

        for key in sorted(all_keys):
            values = {}
            for model_name, metrics in all_metrics.items():
                if key in metrics:
                    values[model_name] = metrics[key]

            if len(values) >= 2:
                best = max(values, key=values.get) if "loss" not in key and "mae" not in key else min(values, key=values.get)
                comparison = " | ".join(f"{n}: {v:.4f}" for n, v in values.items())
                marker = " â† best" if len(values) > 1 else ""
                logger.info("  %-25s %s  [%s%s]", key, comparison, best, marker)

        # ë¹„êµ ê²°ê³¼ ì €ì¥
        import json
        compare_path = f"{s.LOG_DIR}/eval_comparison.json"
        with open(compare_path, "w") as f:
            json.dump(all_metrics, f, indent=2, ensure_ascii=False)
        logger.info("ë¹„êµ ë¦¬í¬íŠ¸ ì €ì¥: %s", compare_path)

    logger.info("")
    logger.info("âœ… í‰ê°€ ì™„ë£Œ")


if __name__ == "__main__":
    main()