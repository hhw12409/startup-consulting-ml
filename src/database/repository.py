"""
ğŸ“ src/database/repository.py
================================
ë°ì´í„° ì €ì¥ì†Œ (Repository íŒ¨í„´).

[ì—­í• ] ê° íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ì˜ DB CRUDë¥¼ ì œê³µí•©ë‹ˆë‹¤.
[íŒ¨í„´] Repository â€” DB ì ‘ê·¼ì„ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ë¡œ ì¶”ìƒí™”

ì €ì¥ì†Œ ëª©ë¡:
    StoreRepository         - stores í…Œì´ë¸” (ì›ë³¸ ë°ì´í„°)
    RegionRepository        - region_codes í…Œì´ë¸” (í–‰ì •ë™ ì½”ë“œ)
    CleanedStoreRepository  - cleaned_stores í…Œì´ë¸” (02_interim)
    LabeledStoreRepository  - labeled_stores í…Œì´ë¸” (03_processed)
    FeatureSetRepository    - feature_sets í…Œì´ë¸” (04_features)
    TrainingRunRepository   - training_runs í…Œì´ë¸” (ì‹¤í—˜ ì¶”ì )
"""

import io as bio
from typing import Optional
from datetime import datetime

import numpy as np
import pandas as pd
from sqlalchemy import func, text, desc
from sqlalchemy.dialects.mysql import insert as mysql_insert

from src.database.connection import get_session
from src.database.models import (
    Store, RegionCode, CollectionLog,
    CleanedStore, LabeledStore, FeatureSet, TrainingRun,
)
from src.utils.logger import get_logger

logger = get_logger(__name__)


# ================================================================
# API ì›ë³¸ ì»¬ëŸ¼ â†’ DB ì»¬ëŸ¼ ë§¤í•‘
# ================================================================
API_TO_DB_MAP = {
    "bizesId": "biz_id",
    "bizesNm": "store_name",
    "brchNm": "branch_name",
    "indsLclsCd": "category_large_cd",
    "indsLclsCdNm": "category_large",
    "indsLclsNm": "category_large",       # CSV í˜¸í™˜ (CdNm ì—†ëŠ” ê²½ìš°)
    "indsMclsCd": "category_mid_cd",
    "indsMclsCdNm": "category_mid",
    "indsMclsNm": "category_mid",          # CSV í˜¸í™˜
    "indsSclsCd": "category_small_cd",
    "indsSclsCdNm": "category_small",
    "indsSclsNm": "category_small",        # CSV í˜¸í™˜
    "ksicCd": "ksic_cd",
    "ksicNm": "ksic_name",
    "ctprvnCd": "sido_cd",
    "ctprvnNm": "sido_name",
    "signguCd": "sgg_cd",
    "signguNm": "sgg_name",
    "adongCd": "adong_cd",
    "adongNm": "adong_name",
    "ldongCd": "ldong_cd",
    "ldongNm": "ldong_name",
    "lnoAdr": "lot_address",
    "rdnmAdr": "road_address",
    "bldNm": "building_name",
    "nwZipCd": "zip_code",
    "lon": "longitude",
    "lat": "latitude",
    "flrNo": "floor_info",
    "hoNo": "unit_info",
    "b_stt_cd": "biz_status_cd",
    "b_stt": "biz_status",
    "end_dt": "closure_date",
}


class StoreRepository:
    """ìƒê°€ ë°ì´í„° ì €ì¥ì†Œ"""

    def upsert_stores(self, df: pd.DataFrame) -> int:
        """
        DataFrame â†’ stores í…Œì´ë¸”ì— UPSERT (ì¤‘ë³µ ì‹œ ì—…ë°ì´íŠ¸).

        Args:
            df: API ìˆ˜ì§‘ ê²°ê³¼ ë˜ëŠ” CSV ë°ì´í„°

        Returns:
            ì €ì¥/ì—…ë°ì´íŠ¸ëœ í–‰ ìˆ˜
        """
        # ì»¬ëŸ¼ ë§¤í•‘
        rename_map = {k: v for k, v in API_TO_DB_MAP.items() if k in df.columns}
        df_mapped = df.rename(columns=rename_map)

        # DB ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
        db_columns = [c.name for c in Store.__table__.columns if c.name not in ("id", "collected_at", "updated_at")]
        valid_cols = [c for c in db_columns if c in df_mapped.columns]
        df_insert = df_mapped[valid_cols].copy()

        # NaN â†’ None
        df_insert = df_insert.where(pd.notnull(df_insert), None)

        session = get_session()
        saved = 0

        try:
            records = df_insert.to_dict("records")

            # ë°°ì¹˜ UPSERT (1000ê±´ì”©)
            for i in range(0, len(records), 1000):
                batch = records[i:i + 1000]

                stmt = mysql_insert(Store).values(batch)

                # ì¤‘ë³µ ì‹œ ì—…ë°ì´íŠ¸í•  ì»¬ëŸ¼
                update_cols = {
                    c: stmt.inserted[c] for c in valid_cols if c != "biz_id"
                }
                update_cols["updated_at"] = datetime.utcnow()

                stmt = stmt.on_duplicate_key_update(**update_cols)

                session.execute(stmt)
                saved += len(batch)

                if (i // 1000 + 1) % 10 == 0:
                    logger.info("  DB ì €ì¥ ì§„í–‰: %d/%dê±´", saved, len(records))

            session.commit()
            logger.info("âœ… DB ì €ì¥ ì™„ë£Œ: %dê±´ (UPSERT)", saved)

        except Exception as e:
            session.rollback()
            logger.error("DB ì €ì¥ ì‹¤íŒ¨: %s", e)
            raise
        finally:
            session.close()

        return saved

    def get_store_count(self) -> int:
        """ì „ì²´ ìƒê°€ ìˆ˜"""
        session = get_session()
        try:
            return session.query(func.count(Store.id)).scalar()
        finally:
            session.close()

    def get_category_stats(self, category: str = None, district: str = None) -> dict:
        """
        ì—…ì¢…/ì§€ì—­ë³„ í†µê³„ ì¡°íšŒ.

        Returns:
            {
                "total": 90174,
                "category_count": 234,
                "category_pct": 36.1,
                "top_sub_categories": [("í•œì‹", 120), ("ì¹´í˜", 80)],
                "survival_rate": 62.0,
                "closure_rate": 38.0,
            }
        """
        session = get_session()
        try:
            total = session.query(func.count(Store.id)).scalar()

            # í•„í„° êµ¬ì„±
            query = session.query(Store)
            if category:
                query = query.filter(Store.category_large.contains(category))
            if district:
                query = query.filter(Store.adong_name.contains(district))

            count = query.count()

            # ì„¸ë¶€ ì—…ì¢… Top 5
            top_subs = (
                query.with_entities(Store.category_mid, func.count(Store.id))
                .group_by(Store.category_mid)
                .order_by(func.count(Store.id).desc())
                .limit(5)
                .all()
            )

            # ì‚¬ì—…ì ìƒíƒœ
            status_counts = (
                query.with_entities(Store.biz_status_cd, func.count(Store.id))
                .group_by(Store.biz_status_cd)
                .all()
            )
            status_dict = dict(status_counts)
            active = status_dict.get("01", 0)
            closed = status_dict.get("03", 0)

            survival = active / (active + closed) * 100 if (active + closed) > 0 else 0
            closure = closed / (active + closed) * 100 if (active + closed) > 0 else 0

            return {
                "total": total,
                "count": count,
                "pct": count / total * 100 if total > 0 else 0,
                "top_sub_categories": [(name, cnt) for name, cnt in top_subs if name],
                "survival_rate": survival,
                "closure_rate": closure,
            }
        finally:
            session.close()

    def get_district_stats(self, district: str) -> dict:
        """ì§€ì—­ë³„ ì—…ì¢… ë¶„í¬"""
        session = get_session()
        try:
            query = session.query(Store).filter(Store.adong_name.contains(district))
            count = query.count()

            top_cats = (
                query.with_entities(Store.category_large, func.count(Store.id))
                .group_by(Store.category_large)
                .order_by(func.count(Store.id).desc())
                .limit(5)
                .all()
            )

            return {
                "count": count,
                "top_categories": [(name, cnt) for name, cnt in top_cats if name],
            }
        finally:
            session.close()

    def to_dataframe(self, category: str = None, district: str = None) -> pd.DataFrame:
        """
        DB â†’ DataFrame ë³€í™˜ (í•™ìŠµ/í”¼ì²˜ë§ìš©).

        Args:
            category: ì—…ì¢… í•„í„°
            district: ì§€ì—­ í•„í„°
        """
        session = get_session()
        try:
            query = session.query(Store)
            if category:
                query = query.filter(Store.category_large.contains(category))
            if district:
                query = query.filter(Store.adong_name.contains(district))

            df = pd.read_sql(query.statement, session.bind)
            logger.info("DB â†’ DataFrame: %dí–‰ Ã— %dì—´", *df.shape)
            return df
        finally:
            session.close()

    def log_collection(self, dong_cd: str, dong_name: str, count: int,
                       status: str = "success", error_msg: str = None):
        """ìˆ˜ì§‘ ì´ë ¥ ì €ì¥"""
        session = get_session()
        try:
            log = CollectionLog(
                dong_cd=dong_cd,
                dong_name=dong_name,
                store_count=count,
                status=status,
                error_msg=error_msg,
            )
            session.add(log)
            session.commit()
        except Exception as e:
            session.rollback()
            logger.error("ìˆ˜ì§‘ ì´ë ¥ ì €ì¥ ì‹¤íŒ¨: %s", e)
        finally:
            session.close()


class RegionRepository:
    """í–‰ì •ë™ ì½”ë“œ ì €ì¥ì†Œ"""

    def upsert_regions(self, df: pd.DataFrame) -> int:
        """í–‰ì •ë™ ì½”ë“œ DataFrame â†’ DB ì €ì¥"""
        col_map = {
            "region_cd": "region_cd",
            "region_cd_8": "region_cd_8",
            "sido_cd": "sido_cd",
            "sgg_cd": "sgg_cd",
            "dong_cd": "dong_cd",
            "sido_nm": "sido_name",
            "sgg_nm": "sgg_name",
            "dong_nm": "dong_name",
            "full_nm": "full_name",
        }

        rename_map = {k: v for k, v in col_map.items() if k in df.columns}
        df_mapped = df.rename(columns=rename_map)

        db_columns = [c.name for c in RegionCode.__table__.columns if c.name != "id"]
        valid_cols = [c for c in db_columns if c in df_mapped.columns]
        df_insert = df_mapped[valid_cols].where(pd.notnull(df_mapped[valid_cols]), None)

        session = get_session()
        saved = 0

        try:
            records = df_insert.to_dict("records")

            for i in range(0, len(records), 500):
                batch = records[i:i + 500]
                stmt = mysql_insert(RegionCode).values(batch)
                stmt = stmt.on_duplicate_key_update(
                    **{c: stmt.inserted[c] for c in valid_cols if c != "region_cd"}
                )
                session.execute(stmt)
                saved += len(batch)

            session.commit()
            logger.info("âœ… í–‰ì •ë™ ì½”ë“œ ì €ì¥: %dê±´", saved)
        except Exception as e:
            session.rollback()
            logger.error("í–‰ì •ë™ ì½”ë“œ ì €ì¥ ì‹¤íŒ¨: %s", e)
            raise
        finally:
            session.close()

        return saved

    def get_dong_codes(self, sido_cd: str = None) -> list[str]:
        """í–‰ì •ë™ 8ìë¦¬ ì½”ë“œ ëª©ë¡ ì¡°íšŒ"""
        session = get_session()
        try:
            query = session.query(RegionCode.region_cd_8)
            if sido_cd:
                query = query.filter(RegionCode.sido_cd == sido_cd)
            # ì‹œë„/ì‹œêµ°êµ¬ ë ˆë²¨ ì œì™¸
            query = query.filter(~RegionCode.region_cd_8.endswith("0000"))
            return [r[0] for r in query.all()]
        finally:
            session.close()


# ================================================================
# numpy ì§ë ¬í™” ìœ í‹¸ë¦¬í‹°
# ================================================================

def _serialize_numpy(arr: np.ndarray) -> bytes:
    """numpy ë°°ì—´ì„ bytesë¡œ ì§ë ¬í™” (DB BLOB ì €ì¥ìš©)"""
    buf = bio.BytesIO()
    np.save(buf, arr)
    return buf.getvalue()


def _deserialize_numpy(data: bytes) -> np.ndarray:
    """bytesë¥¼ numpy ë°°ì—´ë¡œ ì—­ì§ë ¬í™”"""
    buf = bio.BytesIO(data)
    return np.load(buf)


# ================================================================
# CleanedStoreRepository â€” ì •ì œ ë°ì´í„° (02_interim)
# ================================================================

class CleanedStoreRepository:
    """ì •ì œëœ ìƒê°€ ë°ì´í„° ì €ì¥ì†Œ"""

    def save_cleaned(self, df: pd.DataFrame, pipeline_run_id: str) -> int:
        """
        ì •ì œëœ DataFrame â†’ cleaned_stores í…Œì´ë¸”ì— ì €ì¥.

        Args:
            df: DataCleaner.clean() ê²°ê³¼
            pipeline_run_id: íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ UUID

        Returns:
            ì €ì¥ëœ í–‰ ìˆ˜
        """
        # DB ì»¬ëŸ¼ì— ë§¤í•‘ ê°€ëŠ¥í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
        db_columns = [
            c.name for c in CleanedStore.__table__.columns
            if c.name not in ("id", "cleaned_at")
        ]
        valid_cols = [c for c in db_columns if c in df.columns]

        df_insert = df[valid_cols].copy()
        df_insert["pipeline_run_id"] = pipeline_run_id

        # NaN â†’ None
        df_insert = df_insert.where(pd.notnull(df_insert), None)

        session = get_session()
        saved = 0

        try:
            records = df_insert.to_dict("records")

            for i in range(0, len(records), 1000):
                batch = records[i:i + 1000]
                session.bulk_insert_mappings(CleanedStore, batch)
                saved += len(batch)

                if (i // 1000 + 1) % 10 == 0:
                    logger.info("  cleaned_stores ì €ì¥ ì§„í–‰: %d/%dê±´", saved, len(records))

            session.commit()
            logger.info("cleaned_stores ì €ì¥ ì™„ë£Œ: %dê±´ (run=%s)", saved, pipeline_run_id[:8])

        except Exception as e:
            session.rollback()
            logger.error("cleaned_stores ì €ì¥ ì‹¤íŒ¨: %s", e)
            raise
        finally:
            session.close()

        return saved

    def to_dataframe(self, pipeline_run_id: str = None) -> pd.DataFrame:
        """
        cleaned_stores â†’ DataFrame ë³€í™˜.

        Args:
            pipeline_run_id: íŠ¹ì • ì‹¤í–‰ ID (Noneì´ë©´ ìµœì‹ )
        """
        session = get_session()
        try:
            query = session.query(CleanedStore)
            if pipeline_run_id:
                query = query.filter(CleanedStore.pipeline_run_id == pipeline_run_id)
            else:
                latest_id = self.get_latest_run_id()
                if latest_id:
                    query = query.filter(CleanedStore.pipeline_run_id == latest_id)

            df = pd.read_sql(query.statement, session.bind)

            # ORM ë©”íƒ€ ì»¬ëŸ¼ ì œê±°
            drop_cols = ["id", "cleaned_at"]
            df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors="ignore")

            logger.info("cleaned_stores â†’ DataFrame: %dí–‰ Ã— %dì—´", *df.shape)
            return df
        finally:
            session.close()

    def get_latest_run_id(self) -> Optional[str]:
        """ìµœì‹  pipeline_run_id ì¡°íšŒ"""
        session = get_session()
        try:
            result = (
                session.query(CleanedStore.pipeline_run_id)
                .order_by(desc(CleanedStore.cleaned_at))
                .first()
            )
            return result[0] if result else None
        finally:
            session.close()

    def delete_old_runs(self, keep_latest: int = 1):
        """ì˜¤ë˜ëœ íŒŒì´í”„ë¼ì¸ run ë°ì´í„°ë¥¼ ì‚­ì œí•˜ê³  ìµœì‹  Nê°œë§Œ ìœ ì§€"""
        session = get_session()
        try:
            runs = (
                session.query(CleanedStore.pipeline_run_id)
                .group_by(CleanedStore.pipeline_run_id)
                .order_by(desc(func.max(CleanedStore.cleaned_at)))
                .all()
            )
            run_ids = [r[0] for r in runs]

            if len(run_ids) <= keep_latest:
                return 0

            old_ids = run_ids[keep_latest:]
            deleted = (
                session.query(CleanedStore)
                .filter(CleanedStore.pipeline_run_id.in_(old_ids))
                .delete(synchronize_session=False)
            )
            session.commit()
            logger.info("cleaned_stores ì •ë¦¬: %dê±´ ì‚­ì œ (%dê°œ run ì œê±°)", deleted, len(old_ids))
            return deleted
        except Exception as e:
            session.rollback()
            logger.error("cleaned_stores ì •ë¦¬ ì‹¤íŒ¨: %s", e)
            raise
        finally:
            session.close()


# ================================================================
# LabeledStoreRepository â€” ë¼ë²¨ë§ ë°ì´í„° (03_processed)
# ================================================================

class LabeledStoreRepository:
    """ë¼ë²¨ë§ëœ ìƒê°€ ë°ì´í„° ì €ì¥ì†Œ"""

    def save_labeled(self, df: pd.DataFrame, pipeline_run_id: str) -> int:
        """
        ë¼ë²¨ë§ëœ DataFrame â†’ labeled_stores í…Œì´ë¸”ì— ì €ì¥.

        Args:
            df: LabelGenerator.generate() ê²°ê³¼
            pipeline_run_id: íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ UUID

        Returns:
            ì €ì¥ëœ í–‰ ìˆ˜
        """
        db_columns = [
            c.name for c in LabeledStore.__table__.columns
            if c.name not in ("id", "cleaned_store_id", "labeled_at")
        ]
        valid_cols = [c for c in db_columns if c in df.columns]

        df_insert = df[valid_cols].copy()
        df_insert["pipeline_run_id"] = pipeline_run_id

        # NaN â†’ None
        df_insert = df_insert.where(pd.notnull(df_insert), None)

        session = get_session()
        saved = 0

        try:
            records = df_insert.to_dict("records")

            for i in range(0, len(records), 1000):
                batch = records[i:i + 1000]
                session.bulk_insert_mappings(LabeledStore, batch)
                saved += len(batch)

                if (i // 1000 + 1) % 10 == 0:
                    logger.info("  labeled_stores ì €ì¥ ì§„í–‰: %d/%dê±´", saved, len(records))

            session.commit()
            logger.info("labeled_stores ì €ì¥ ì™„ë£Œ: %dê±´ (run=%s)", saved, pipeline_run_id[:8])

        except Exception as e:
            session.rollback()
            logger.error("labeled_stores ì €ì¥ ì‹¤íŒ¨: %s", e)
            raise
        finally:
            session.close()

        return saved

    def to_dataframe(self, pipeline_run_id: str = None) -> pd.DataFrame:
        """
        labeled_stores â†’ DataFrame ë³€í™˜.

        Args:
            pipeline_run_id: íŠ¹ì • ì‹¤í–‰ ID (Noneì´ë©´ ìµœì‹ )
        """
        session = get_session()
        try:
            query = session.query(LabeledStore)
            if pipeline_run_id:
                query = query.filter(LabeledStore.pipeline_run_id == pipeline_run_id)
            else:
                latest_id = self.get_latest_run_id()
                if latest_id:
                    query = query.filter(LabeledStore.pipeline_run_id == latest_id)

            df = pd.read_sql(query.statement, session.bind)

            # ORM ë©”íƒ€ ì»¬ëŸ¼ ì œê±°
            drop_cols = ["id", "cleaned_store_id", "labeled_at"]
            df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors="ignore")

            logger.info("labeled_stores â†’ DataFrame: %dí–‰ Ã— %dì—´", *df.shape)
            return df
        finally:
            session.close()

    def get_latest_run_id(self) -> Optional[str]:
        """ìµœì‹  pipeline_run_id ì¡°íšŒ"""
        session = get_session()
        try:
            result = (
                session.query(LabeledStore.pipeline_run_id)
                .order_by(desc(LabeledStore.labeled_at))
                .first()
            )
            return result[0] if result else None
        finally:
            session.close()

    def delete_old_runs(self, keep_latest: int = 1):
        """ì˜¤ë˜ëœ íŒŒì´í”„ë¼ì¸ run ë°ì´í„°ë¥¼ ì‚­ì œí•˜ê³  ìµœì‹  Nê°œë§Œ ìœ ì§€"""
        session = get_session()
        try:
            runs = (
                session.query(LabeledStore.pipeline_run_id)
                .group_by(LabeledStore.pipeline_run_id)
                .order_by(desc(func.max(LabeledStore.labeled_at)))
                .all()
            )
            run_ids = [r[0] for r in runs]

            if len(run_ids) <= keep_latest:
                return 0

            old_ids = run_ids[keep_latest:]
            deleted = (
                session.query(LabeledStore)
                .filter(LabeledStore.pipeline_run_id.in_(old_ids))
                .delete(synchronize_session=False)
            )
            session.commit()
            logger.info("labeled_stores ì •ë¦¬: %dê±´ ì‚­ì œ (%dê°œ run ì œê±°)", deleted, len(old_ids))
            return deleted
        except Exception as e:
            session.rollback()
            logger.error("labeled_stores ì •ë¦¬ ì‹¤íŒ¨: %s", e)
            raise
        finally:
            session.close()


# ================================================================
# FeatureSetRepository â€” í”¼ì²˜ì…‹ (04_features)
# ================================================================

class FeatureSetRepository:
    """í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ê²°ê³¼ ì €ì¥ì†Œ"""

    def save_feature_set(
        self,
        X: np.ndarray,
        y: np.ndarray,
        feature_columns: list[str],
        target_columns: list[str],
        pipeline_run_id: str,
        scaler_params: dict = None,
        encoder_classes: dict = None,
        source_row_count: int = None,
    ) -> int:
        """
        í”¼ì²˜ ë°°ì—´ â†’ feature_sets í…Œì´ë¸”ì— ì €ì¥.

        numpy ë°°ì—´ì€ BLOBìœ¼ë¡œ ì§ë ¬í™”í•˜ê³ ,
        ë©”íƒ€ë°ì´í„°(ì»¬ëŸ¼ëª…, ìŠ¤ì¼€ì¼ëŸ¬ íŒŒë¼ë¯¸í„° ë“±)ëŠ” JSONìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            X: í”¼ì²˜ ë°°ì—´ [N, features]
            y: íƒ€ê²Ÿ ë°°ì—´ [N, targets]
            feature_columns: í”¼ì²˜ ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸
            target_columns: íƒ€ê²Ÿ ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸
            pipeline_run_id: íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ UUID
            scaler_params: StandardScaler íŒŒë¼ë¯¸í„° dict
            encoder_classes: LabelEncoder í´ë˜ìŠ¤ dict

        Returns:
            ì €ì¥ëœ FeatureSet ID
        """
        session = get_session()
        try:
            feature_set = FeatureSet(
                pipeline_run_id=pipeline_run_id,
                feature_columns=feature_columns,
                target_columns=target_columns,
                n_samples=X.shape[0],
                n_features=X.shape[1],
                n_targets=y.shape[1] if y.ndim > 1 else 1,
                feature_data=_serialize_numpy(X),
                target_data=_serialize_numpy(y),
                scaler_params=scaler_params,
                encoder_classes=encoder_classes,
                source_row_count=source_row_count or X.shape[0],
            )
            session.add(feature_set)
            session.commit()

            logger.info(
                "feature_sets ì €ì¥: %d samples Ã— %d features (run=%s)",
                X.shape[0], X.shape[1], pipeline_run_id[:8],
            )
            return feature_set.id

        except Exception as e:
            session.rollback()
            logger.error("feature_sets ì €ì¥ ì‹¤íŒ¨: %s", e)
            raise
        finally:
            session.close()

    def load_feature_set(
        self, pipeline_run_id: str = None
    ) -> tuple[np.ndarray, np.ndarray, list[str], list[str]]:
        """
        feature_sets â†’ numpy ë°°ì—´ + ë©”íƒ€ë°ì´í„° ë¡œë“œ.

        Args:
            pipeline_run_id: íŠ¹ì • ì‹¤í–‰ ID (Noneì´ë©´ ìµœì‹ )

        Returns:
            (X, y, feature_columns, target_columns)
        """
        session = get_session()
        try:
            query = session.query(FeatureSet)
            if pipeline_run_id:
                query = query.filter(FeatureSet.pipeline_run_id == pipeline_run_id)

            feature_set = query.order_by(desc(FeatureSet.created_at)).first()

            if not feature_set:
                raise ValueError("ì €ì¥ëœ í”¼ì²˜ì…‹ì´ ì—†ìŠµë‹ˆë‹¤")

            X = _deserialize_numpy(feature_set.feature_data)
            y = _deserialize_numpy(feature_set.target_data)

            logger.info(
                "feature_sets ë¡œë“œ: %d samples Ã— %d features (run=%s)",
                X.shape[0], X.shape[1], feature_set.pipeline_run_id[:8],
            )
            return X, y, feature_set.feature_columns, feature_set.target_columns

        finally:
            session.close()

    def get_latest_run_id(self) -> Optional[str]:
        """ìµœì‹  pipeline_run_id ì¡°íšŒ"""
        session = get_session()
        try:
            result = (
                session.query(FeatureSet.pipeline_run_id)
                .order_by(desc(FeatureSet.created_at))
                .first()
            )
            return result[0] if result else None
        finally:
            session.close()


# ================================================================
# TrainingRunRepository â€” í•™ìŠµ ì‹¤í–‰ ì´ë ¥
# ================================================================

class TrainingRunRepository:
    """í•™ìŠµ ì‹¤í–‰ ì´ë ¥ ì €ì¥ì†Œ"""

    def create_run(
        self,
        run_id: str,
        model_type: str,
        pipeline_run_id: str = None,
        model_name: str = None,
        train_size: int = None,
        val_size: int = None,
        test_size: int = None,
        n_features: int = None,
        hyperparameters: dict = None,
    ) -> TrainingRun:
        """
        í•™ìŠµ ì‹¤í–‰ ê¸°ë¡ì„ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            run_id: í•™ìŠµ ì‹¤í–‰ UUID
            model_type: ëª¨ë¸ íƒ€ì… (xgboost, neural_net)
            pipeline_run_id: ì‚¬ìš©í•œ í”¼ì²˜ì…‹ì˜ íŒŒì´í”„ë¼ì¸ ID

        Returns:
            ìƒì„±ëœ TrainingRun ê°ì²´
        """
        session = get_session()
        try:
            run = TrainingRun(
                run_id=run_id,
                pipeline_run_id=pipeline_run_id,
                model_type=model_type,
                model_name=model_name,
                train_size=train_size,
                val_size=val_size,
                test_size=test_size,
                n_features=n_features,
                hyperparameters=hyperparameters,
                status="started",
            )
            session.add(run)
            session.commit()
            session.refresh(run)
            session.expunge(run)

            logger.info("í•™ìŠµ ì‹¤í–‰ ìƒì„±: %s (%s)", run_id[:8], model_type)
            return run

        except Exception as e:
            session.rollback()
            logger.error("í•™ìŠµ ì‹¤í–‰ ìƒì„± ì‹¤íŒ¨: %s", e)
            raise
        finally:
            session.close()

    def update_status(
        self,
        run_id: str,
        status: str,
        metrics: dict = None,
        model_path: str = None,
        artifacts_path: str = None,
        error_message: str = None,
    ):
        """
        í•™ìŠµ ì‹¤í–‰ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

        Args:
            run_id: í•™ìŠµ ì‹¤í–‰ UUID
            status: ìƒˆ ìƒíƒœ (training, evaluating, completed, failed)
            metrics: í‰ê°€ ê²°ê³¼ dict
        """
        session = get_session()
        try:
            run = session.query(TrainingRun).filter(TrainingRun.run_id == run_id).first()
            if not run:
                logger.error("í•™ìŠµ ì‹¤í–‰ ì—†ìŒ: %s", run_id)
                return

            run.status = status
            if metrics:
                run.metrics = metrics
            if model_path:
                run.model_path = model_path
            if artifacts_path:
                run.artifacts_path = artifacts_path
            if error_message:
                run.error_message = error_message
            if status in ("completed", "failed"):
                run.completed_at = datetime.utcnow()

            session.commit()
            logger.info("í•™ìŠµ ì‹¤í–‰ ì—…ë°ì´íŠ¸: %s â†’ %s", run_id[:8], status)

        except Exception as e:
            session.rollback()
            logger.error("í•™ìŠµ ì‹¤í–‰ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: %s", e)
            raise
        finally:
            session.close()

    def get_run(self, run_id: str) -> Optional[TrainingRun]:
        """íŠ¹ì • í•™ìŠµ ì‹¤í–‰ ì¡°íšŒ"""
        session = get_session()
        try:
            run = session.query(TrainingRun).filter(TrainingRun.run_id == run_id).first()
            if run:
                session.expunge(run)
            return run
        finally:
            session.close()

    def get_latest_run(self, model_type: str = None) -> Optional[TrainingRun]:
        """ìµœì‹  í•™ìŠµ ì‹¤í–‰ ì¡°íšŒ"""
        session = get_session()
        try:
            query = session.query(TrainingRun).filter(TrainingRun.status == "completed")
            if model_type:
                query = query.filter(TrainingRun.model_type == model_type)
            run = query.order_by(desc(TrainingRun.completed_at)).first()
            if run:
                session.expunge(run)
            return run
        finally:
            session.close()