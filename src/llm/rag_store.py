"""
ğŸ“ src/llm/rag_store.py
=========================
RAG ë²¡í„°ìŠ¤í† ì–´ â€” ChromaDB + Ollama ì„ë² ë”©.

[ì—­í• ] ìˆ˜ì§‘ëœ ìƒê°€ ë°ì´í„°ë¥¼ ë²¡í„°DBì— ì €ì¥í•˜ê³ ,
       ì‚¬ìš©ì ì§ˆë¬¸ì— ê´€ë ¨ëœ ìƒê°€ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…í•©ë‹ˆë‹¤.

[íë¦„]
  êµ¬ì¶•: stores_raw.csv â†’ í…ìŠ¤íŠ¸ ë³€í™˜ â†’ Ollama ì„ë² ë”© â†’ ChromaDB ì €ì¥
  ê²€ìƒ‰: ì§ˆë¬¸ â†’ Ollama ì„ë² ë”© â†’ ChromaDB ìœ ì‚¬ë„ ê²€ìƒ‰ â†’ TOP-K ê²°ê³¼

[ì„¤ì¹˜]
  pip install chromadb
  ollama pull nomic-embed-text   # ì„ë² ë”© ì „ìš© ëª¨ë¸ (274MB)

ì‚¬ìš©ë²•:
    store = RAGStore()
    store.build()                             # 1íšŒ: ë²¡í„°DB êµ¬ì¶•
    results = store.search("ê°•ë‚¨êµ¬ ì¹´í˜ í˜„í™©")  # ê´€ë ¨ ìƒê°€ ê²€ìƒ‰
"""

import os
import json
import requests
import pandas as pd
from pathlib import Path
from typing import Optional

from config.settings import get_settings
from src.utils.logger import get_logger

logger = get_logger(__name__)

# Ollama ì„ë² ë”© ëª¨ë¸ (ê²½ëŸ‰, ë¹ ë¦„)
EMBED_MODEL = "nomic-embed-text"
OLLAMA_URL = "http://localhost:11434"

# ChromaDB ì €ì¥ ê²½ë¡œ
CHROMA_DIR = "data/06_vector_db"
COLLECTION_NAME = "stores"


class RAGStore:
    """
    RAG ë²¡í„°ìŠ¤í† ì–´.

    ì‚¬ìš©ë²•:
        store = RAGStore()

        # 1íšŒ: ë²¡í„°DB êµ¬ì¶• (stores_raw.csv â†’ ChromaDB)
        store.build()

        # ê²€ìƒ‰: ì§ˆë¬¸ì— ê´€ë ¨ëœ ìƒê°€ ë°ì´í„° ë°˜í™˜
        results = store.search("ê°•ë‚¨êµ¬ì—ì„œ ì¹´í˜ ì°½ì—…í•˜ë ¤ë©´?", top_k=5)

        # í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ ë°˜í™˜
        context = store.get_rag_context("ê°•ë‚¨êµ¬ ì¹´í˜ ê²½ìŸ í˜„í™©")
    """

    def __init__(self, chroma_dir: str = None):
        self._chroma_dir = chroma_dir or CHROMA_DIR
        self._collection = None

    # ================================================================
    # ë²¡í„°DB êµ¬ì¶•
    # ================================================================

    def build(self, batch_size: int = 100, max_docs: int = None) -> int:
        """
        stores_raw.csv â†’ ChromaDB ë²¡í„°DB êµ¬ì¶•.

        Args:
            batch_size: ì„ë² ë”© ë°°ì¹˜ í¬ê¸° (Ollama í˜¸ì¶œ ë‹¨ìœ„)
            max_docs: ìµœëŒ€ ë¬¸ì„œ ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©, None=ì „ì²´)

        Returns:
            ì €ì¥ëœ ë¬¸ì„œ ìˆ˜
        """
        # 1. ë°ì´í„° ë¡œë“œ
        df = self._load_data()
        if df.empty:
            return 0

        if max_docs:
            df = df.head(max_docs)
            logger.info("max_docs=%d ì ìš©", max_docs)

        # 2. í…ìŠ¤íŠ¸ ë³€í™˜
        docs = self._to_documents(df)
        logger.info("ë¬¸ì„œ ë³€í™˜ ì™„ë£Œ: %dê±´", len(docs))

        # 3. ChromaDB ì»¬ë ‰ì…˜ ìƒì„±
        collection = self._get_or_create_collection(reset=True)

        # 4. ë°°ì¹˜ ì„ë² ë”© & ì €ì¥
        total_saved = 0
        total_batches = (len(docs) + batch_size - 1) // batch_size

        for i in range(0, len(docs), batch_size):
            batch_num = i // batch_size + 1
            batch = docs[i:i + batch_size]

            texts = [d["text"] for d in batch]
            ids = [d["id"] for d in batch]
            metadatas = [d["metadata"] for d in batch]

            try:
                # Ollama ì„ë² ë”©
                embeddings = self._embed_batch(texts)

                if embeddings:
                    collection.add(
                        documents=texts,
                        embeddings=embeddings,
                        ids=ids,
                        metadatas=metadatas,
                    )
                    total_saved += len(batch)

                if batch_num % 50 == 0 or batch_num == total_batches:
                    logger.info("  [%d/%d] ë°°ì¹˜ ì €ì¥ ì™„ë£Œ (%dê±´ ëˆ„ì )", batch_num, total_batches, total_saved)

            except Exception as e:
                logger.error("ë°°ì¹˜ %d ì‹¤íŒ¨: %s", batch_num, e)
                # ì—°ì† ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
                if total_saved == 0 and batch_num >= 3:
                    logger.error("âš ï¸ ì—°ì† ì‹¤íŒ¨ â†’ êµ¬ì¶• ì¤‘ë‹¨. Ollama ì„ë² ë”© ëª¨ë¸ í™•ì¸ í•„ìš”")
                    logger.error("  ì„¤ì¹˜: ollama pull nomic-embed-text")
                    break

        logger.info("â”â”â” RAG ë²¡í„°DB êµ¬ì¶• ì™„ë£Œ: %dê±´ ì €ì¥ â”â”â”", total_saved)
        return total_saved

    # ================================================================
    # ê²€ìƒ‰
    # ================================================================

    def search(self, query: str, top_k: int = 5) -> list[dict]:
        """
        ì§ˆë¬¸ì— ê´€ë ¨ëœ ìƒê°€ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.

        Args:
            query: ê²€ìƒ‰ ì§ˆë¬¸
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜

        Returns:
            [{"text": "...", "metadata": {...}, "distance": 0.23}, ...]
        """
        collection = self._get_or_create_collection()

        if collection.count() == 0:
            logger.warning("ë²¡í„°DBê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë¨¼ì € build()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return []

        try:
            query_embedding = self._embed_text(query)
            if not query_embedding:
                return []

            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
            )

            items = []
            for i in range(len(results["ids"][0])):
                items.append({
                    "text": results["documents"][0][i],
                    "metadata": results["metadatas"][0][i] if results["metadatas"] else {},
                    "distance": results["distances"][0][i] if results["distances"] else 0,
                })

            logger.debug("RAG ê²€ìƒ‰ '%s' â†’ %dê±´", query[:30], len(items))
            return items

        except Exception as e:
            logger.error("RAG ê²€ìƒ‰ ì‹¤íŒ¨: %s", e)
            return []

    def get_rag_context(self, query: str, top_k: int = 5) -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            query: ê²€ìƒ‰ ì§ˆë¬¸
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜

        Returns:
            í”„ë¡¬í”„íŠ¸ì— ì‚½ì…í•  í…ìŠ¤íŠ¸
        """
        results = self.search(query, top_k)

        if not results:
            return ""

        lines = ["## ğŸ” ìœ ì‚¬ ì‚¬ë¡€ ë°ì´í„° (RAG ê²€ìƒ‰ ê²°ê³¼)\n"]
        for i, r in enumerate(results, 1):
            lines.append(f"**ì‚¬ë¡€ {i}** (ìœ ì‚¬ë„: {1 - r['distance']:.2f})")
            lines.append(r["text"])
            lines.append("")

        lines.append("ìœ„ ìœ ì‚¬ ì‚¬ë¡€ë¥¼ ì°¸ê³ í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”.\n")

        return "\n".join(lines)

    @property
    def doc_count(self) -> int:
        """ì €ì¥ëœ ë¬¸ì„œ ìˆ˜"""
        try:
            collection = self._get_or_create_collection()
            return collection.count()
        except Exception:
            return 0

    # ================================================================
    # ë‚´ë¶€ ë©”ì„œë“œ
    # ================================================================

    def _load_data(self) -> pd.DataFrame:
        """DB(stores í…Œì´ë¸”)ì—ì„œ ìƒê°€ ë°ì´í„° ë¡œë“œ"""
        try:
            from src.database.repository import StoreRepository
            repo = StoreRepository()
            df = repo.to_dataframe()
            if not df.empty:
                # DB ì»¬ëŸ¼ì€ ì´ë¯¸ ë¬¸ìì—´ì´ ì•„ë‹ ìˆ˜ ìˆìœ¼ë¯€ë¡œ str ë³€í™˜
                df = df.astype(str)
                logger.info("DBì—ì„œ ë°ì´í„° ë¡œë“œ: %dê±´", len(df))
                return df
        except Exception as e:
            logger.warning("DB ë¡œë“œ ì‹¤íŒ¨: %s", e)

        logger.error("ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 'make collect'ë¡œ ìˆ˜ì§‘í•˜ì„¸ìš”.")
        return pd.DataFrame()

    def _to_documents(self, df: pd.DataFrame) -> list[dict]:
        """DataFrame â†’ ë²¡í„°DB ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        docs = []

        # ì»¬ëŸ¼ëª… íƒìƒ‰ (DB ì»¬ëŸ¼ëª… ìš°ì„ )
        name_col = self._find_col(df, ["store_name", "bizesNm"])
        cat_col = self._find_col(df, ["category_large", "business_category", "indsLclsCdNm"])
        sub_col = self._find_col(df, ["category_mid", "business_sub_category", "indsMclsCdNm"])
        detail_col = self._find_col(df, ["category_small", "indsSclsCdNm", "business_detail"])
        dist_col = self._find_col(df, ["adong_name", "district", "adongNm"])
        addr_col = self._find_col(df, ["road_address", "rdnmAdr", "lnoAdr"])
        sgg_col = self._find_col(df, ["sgg_name", "sggNm", "sgg_nm"])
        status_col = self._find_col(df, ["biz_status_cd", "b_stt_cd", "b_stt"])

        for idx, row in df.iterrows():
            name = row.get(name_col, "ìƒí˜¸ ë¯¸ìƒ") if name_col else "ìƒí˜¸ ë¯¸ìƒ"
            cat = row.get(cat_col, "") if cat_col else ""
            sub = row.get(sub_col, "") if sub_col else ""
            detail = row.get(detail_col, "") if detail_col else ""
            dist = row.get(dist_col, "") if dist_col else ""
            addr = row.get(addr_col, "") if addr_col else ""
            sgg = row.get(sgg_col, "") if sgg_col else ""

            # ì‚¬ì—…ì ìƒíƒœ
            status = ""
            if status_col:
                code = row.get(status_col, "")
                status_map = {"01": "ì˜ì—…ì¤‘", "02": "íœ´ì—…", "03": "íì—…"}
                status = status_map.get(str(code), "")

            # í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            parts = [f"{name}"]
            if cat:
                parts.append(f"ì—…ì¢…: {cat}")
            if sub:
                parts.append(f"ì„¸ë¶€: {sub}")
            if detail:
                parts.append(f"ìƒì„¸: {detail}")
            if sgg and dist:
                parts.append(f"ìœ„ì¹˜: {sgg} {dist}")
            elif dist:
                parts.append(f"ìœ„ì¹˜: {dist}")
            if addr and str(addr) != "nan":
                parts.append(f"ì£¼ì†Œ: {addr}")
            if status:
                parts.append(f"ìƒíƒœ: {status}")

            text = " | ".join(parts)

            # ë©”íƒ€ë°ì´í„°
            metadata = {}
            if cat:
                metadata["category"] = str(cat)
            if sub:
                metadata["sub_category"] = str(sub)
            if dist:
                metadata["district"] = str(dist)
            if sgg:
                metadata["sgg"] = str(sgg)
            if status:
                metadata["status"] = status

            docs.append({
                "id": f"store_{idx}",
                "text": text,
                "metadata": metadata,
            })

        return docs

    def _get_or_create_collection(self, reset: bool = False):
        """ChromaDB ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°/ìƒì„±"""
        import chromadb

        client = chromadb.PersistentClient(path=self._chroma_dir)

        if reset:
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ì¬ìƒì„±
            try:
                client.delete_collection(COLLECTION_NAME)
            except Exception:
                pass

        collection = client.get_or_create_collection(
            name=COLLECTION_NAME,
            metadata={"hnsw:space": "cosine"},  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        )

        return collection

    def _embed_text(self, text: str) -> list[float]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© (Ollama)"""
        try:
            resp = requests.post(
                f"{OLLAMA_URL}/api/embeddings",
                json={"model": EMBED_MODEL, "prompt": text},
                timeout=30,
            )
            resp.raise_for_status()
            return resp.json().get("embedding", [])
        except Exception as e:
            logger.error("ì„ë² ë”© ì‹¤íŒ¨: %s", e)
            return []

    def _embed_batch(self, texts: list[str]) -> list[list[float]]:
        """ë°°ì¹˜ ì„ë² ë”© (OllamaëŠ” ê°œë³„ í˜¸ì¶œ)"""
        embeddings = []
        for text in texts:
            emb = self._embed_text(text)
            if emb:
                embeddings.append(emb)
            else:
                # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë²¡í„° ëŒ€ì‹  ì—ëŸ¬
                return []
        return embeddings

    def _find_col(self, df: pd.DataFrame, candidates: list[str]) -> str:
        """DataFrameì—ì„œ ì¡´ì¬í•˜ëŠ” ì²« ë²ˆì§¸ ì»¬ëŸ¼ëª… ë°˜í™˜"""
        for col in candidates:
            if col in df.columns:
                return col
        return ""