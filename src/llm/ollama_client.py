"""
ğŸ“ src/llm/ollama_client.py
==============================
Ollama ë¡œì»¬ LLM í´ë¼ì´ì–¸íŠ¸.

[íŒ¨í„´] Strategy â€” BaseLLM êµ¬í˜„ì²´
[ì—­í• ] ì™¸ë¶€ API ì—†ì´ ë¡œì»¬ì—ì„œ LLMì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

ì„¤ì¹˜:
  1. brew install ollama (macOS) ë˜ëŠ” https://ollama.com
  2. ollama pull gemma2:9b       (ì¶”ì²œ: í•œêµ­ì–´ ì¢‹ìŒ, 32GB Mac)
  3. ollama serve                (ì„œë²„ ì‹œì‘)

í•œêµ­ì–´ ëª¨ë¸ ìš°ì„ ìˆœìœ„ (32GB Mac ê¸°ì¤€):
  1ìˆœìœ„: EEVE-Korean-10.8B    â€” í•œêµ­ì–´ ìµœê³ , 6.5GB
  2ìˆœìœ„: gemma2:9b            â€” í•œêµ­ì–´ ì¢‹ìŒ, 5.4GB
  3ìˆœìœ„: llama3.1:8b          â€” í•œêµ­ì–´ ë³´í†µ, 4.7GB
  4ìˆœìœ„: gemma2:2b            â€” ê°€ë²¼ì›€, 1.6GB (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ)

ì‚¬ìš©ë²•:
    client = OllamaClient()
    if client.is_available():
        response = client.generate("ì°½ì—… ë¶„ì„í•´ì£¼ì„¸ìš”")
        print(client.name)  # "Ollama (EEVE-Korean-10.8B)"
"""

import requests
from typing import Optional

from src.llm.base import BaseLLM
from src.utils.logger import get_logger

logger = get_logger(__name__)


# í•œêµ­ì–´ ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì •ë ¬ (32GB Mac ê¸°ì¤€)
KOREAN_MODEL_PRIORITY = [
    "EEVE-Korean-10.8B",           # í•œêµ­ì–´ íŠ¹í™”, ìµœê³  í’ˆì§ˆ
    "eeve-korean-10.8b:latest",    # íƒœê·¸ í˜•ì‹
    "gemma2:9b",                   # êµ¬ê¸€, í•œêµ­ì–´ ì¢‹ìŒ
    "gemma2:latest",               # gemma2 ê¸°ë³¸
    "llama3.1:8b",                 # Meta, í•œêµ­ì–´ ë³´í†µ
    "llama3.1:latest",
    "gemma2:2b",                   # ê²½ëŸ‰ í´ë°±
    "llama3.2:3b",                 # ê²½ëŸ‰ í´ë°±
]


class OllamaClient(BaseLLM):
    """
    Ollama ë¡œì»¬ LLM í´ë¼ì´ì–¸íŠ¸.

    í•œêµ­ì–´ ëª¨ë¸ì„ ìë™ íƒì§€í•˜ì—¬ ìµœì ì˜ ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤.
    """

    DEFAULT_URL = "http://localhost:11434"

    def __init__(self, model: str = None, base_url: str = None):
        self._url = base_url or self.DEFAULT_URL
        self._model = model  # Noneì´ë©´ ìë™ íƒì§€
        self._available = None

    @property
    def name(self) -> str:
        model = self._model or "ë¯¸íƒì§€"
        return f"Ollama ({model})"

    def is_available(self) -> bool:
        """Ollama ì„œë²„ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸ + ìµœì  ëª¨ë¸ ìë™ íƒì§€"""
        try:
            resp = requests.get(f"{self._url}/api/tags", timeout=3)
            if resp.status_code != 200:
                return False

            # ëª¨ë¸ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ìë™ íƒì§€
            if not self._model:
                self._model = self._find_best_model(resp.json())

            self._available = self._model is not None
            return self._available

        except requests.ConnectionError:
            self._available = False
            return False

    def _find_best_model(self, tags_response: dict) -> Optional[str]:
        """
        ì„¤ì¹˜ëœ ëª¨ë¸ ì¤‘ í•œêµ­ì–´ ìµœì  ëª¨ë¸ì„ ì°¾ìŠµë‹ˆë‹¤.

        ìš°ì„ ìˆœìœ„: EEVE-Korean > gemma2:9b > llama3.1:8b > ê¸°íƒ€
        """
        installed = []
        for m in tags_response.get("models", []):
            name = m.get("name", "")
            installed.append(name)

        if not installed:
            logger.warning("Ollamaì— ì„¤ì¹˜ëœ ëª¨ë¸ ì—†ìŒ. 'ollama pull gemma2:9b' ì‹¤í–‰ í•„ìš”")
            return None

        logger.info("Ollama ì„¤ì¹˜ëœ ëª¨ë¸: %s", installed)

        # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ë§¤ì¹­
        for priority_model in KOREAN_MODEL_PRIORITY:
            for inst in installed:
                if priority_model.lower() in inst.lower():
                    logger.info("âœ… ìµœì  í•œêµ­ì–´ ëª¨ë¸ ì„ íƒ: %s", inst)
                    return inst

        # ìš°ì„ ìˆœìœ„ì— ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ëª¨ë¸ ì‚¬ìš©
        fallback = installed[0]
        logger.info("í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ ì—†ìŒ â†’ í´ë°±: %s", fallback)
        return fallback

    def list_models(self) -> list[str]:
        """ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        try:
            resp = requests.get(f"{self._url}/api/tags", timeout=3)
            if resp.status_code == 200:
                return [m["name"] for m in resp.json().get("models", [])]
        except requests.ConnectionError:
            pass
        return []

    def generate(
            self,
            prompt: str,
            system: Optional[str] = None,
            max_tokens: int = 2000,
            temperature: float = 0.7,
    ) -> str:
        if not self._model:
            if not self.is_available():
                raise RuntimeError("Ollama ì„œë²„ ë¯¸ì‹¤í–‰. 'ollama serve'ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")

        payload = {
            "model": self._model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            },
        }
        if system:
            payload["system"] = system

        try:
            resp = requests.post(
                f"{self._url}/api/generate",
                json=payload,
                timeout=180,  # í° ëª¨ë¸ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŒ
            )
            resp.raise_for_status()
            text = resp.json().get("response", "")
            logger.debug("Ollama ì‘ë‹µ (%s): %dì", self._model, len(text))
            return text

        except Exception as e:
            logger.error("Ollama í˜¸ì¶œ ì‹¤íŒ¨ (%s): %s", self._model, e)
            raise