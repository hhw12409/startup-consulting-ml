"""
ğŸ“ src/serving/app.py
======================
FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜.

ì—”ë“œí¬ì¸íŠ¸:
  GET  /health              â†’ ì„œë²„ ìƒíƒœ
  POST /api/v1/predict      â†’ ML ì˜ˆì¸¡ (ìˆ«ì)
  POST /api/v1/consult      â†’ ML ì˜ˆì¸¡ + LLM ì»¨ì„¤íŒ… ë¦¬í¬íŠ¸
  POST /api/v1/strategy     â†’ ë§ì¶¤í˜• ì „ëµ ì œì•ˆ
  POST /api/v1/ask          â†’ Q&A ëŒ€í™”
  POST /api/v1/competitors  â†’ ê²½ìŸì—…ì²´ ë¶„ì„

ì‹¤í–‰: uvicorn src.serving.app:app --reload
ë¬¸ì„œ: http://localhost:8000/docs
"""

from contextlib import asynccontextmanager
from fastapi import FastAPI, Depends, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Optional

from config.settings import get_settings
from src.utils.logger import setup_logging, get_logger
from src.serving.schemas import PredictionRequest, PredictionResponse
from src.serving.predictor import Predictor
from src.serving.dependencies import get_predictor, get_consultant

logger = get_logger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    setup_logging()
    logger.info("ğŸš€ ì„œë²„ ì‹œì‘: %s", get_settings().APP_VERSION)
    yield
    logger.info("ğŸ‘‹ ì„œë²„ ì¢…ë£Œ")


app = FastAPI(
    title="ì°½ì—… ì»¨ì„¤í„´íŠ¸ AI API",
    description="ML ì˜ˆì¸¡ + LLM ê¸°ë°˜ ì°½ì—… ì»¨ì„¤íŒ… ì„œë¹„ìŠ¤",
    version=get_settings().APP_VERSION,
    lifespan=lifespan,
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)


# â”€â”€ ì¶”ê°€ ìŠ¤í‚¤ë§ˆ â”€â”€

class QARequest(BaseModel):
    """Q&A ìš”ì²­"""
    question: str = Field(..., description="ì§ˆë¬¸")
    prediction_input: PredictionRequest
    chat_history: Optional[list[dict]] = Field(default=None, description="ì´ì „ ëŒ€í™” ê¸°ë¡")


class LLMResponse(BaseModel):
    """LLM ì‘ë‹µ"""
    success: bool = True
    llm_provider: str = ""
    prediction: Optional[dict] = None
    analysis: str = ""
    error: Optional[str] = None


# â”€â”€ ì—”ë“œí¬ì¸íŠ¸ â”€â”€

@app.get("/health")
async def health():
    from src.serving.dependencies import get_consultant
    consultant = get_consultant()
    return {
        "status": "ok",
        "version": get_settings().APP_VERSION,
        "llm": consultant.active_llm,
    }


@app.post("/api/v1/predict", response_model=PredictionResponse)
async def predict(
        req: PredictionRequest,
        predictor: Predictor = Depends(get_predictor),
):
    """ML ì˜ˆì¸¡ë§Œ (ìˆ«ì ê²°ê³¼)"""
    try:
        result = predictor.predict(req.to_dict())
        return PredictionResponse(success=True, data=result)
    except Exception as e:
        logger.error("ì˜ˆì¸¡ ì‹¤íŒ¨: %s", e, exc_info=True)
        raise HTTPException(500, detail=str(e))


@app.post("/api/v1/consult", response_model=LLMResponse)
async def consult(req: PredictionRequest):
    """ML ì˜ˆì¸¡ + LLM ì¢…í•© ì»¨ì„¤íŒ… ë¦¬í¬íŠ¸"""
    try:
        predictor = get_predictor()
        consultant = get_consultant()

        input_data = req.to_dict()
        prediction = predictor.predict(input_data)

        # ì›ë³¸ ì…ë ¥ì— API ìŠ¤í‚¤ë§ˆ í•„ë“œë„ í¬í•¨
        full_input = {**input_data, **req.model_dump()}

        report = consultant.generate_report(full_input, prediction)

        return LLMResponse(
            success=True,
            llm_provider=consultant.active_llm,
            prediction=prediction,
            analysis=report,
        )
    except Exception as e:
        logger.error("ì»¨ì„¤íŒ… ì‹¤íŒ¨: %s", e, exc_info=True)
        raise HTTPException(500, detail=str(e))


@app.post("/api/v1/strategy", response_model=LLMResponse)
async def strategy(req: PredictionRequest):
    """ë§ì¶¤í˜• ì „ëµ ì œì•ˆ"""
    try:
        predictor = get_predictor()
        consultant = get_consultant()

        input_data = req.to_dict()
        prediction = predictor.predict(input_data)
        full_input = {**input_data, **req.model_dump()}

        result = consultant.suggest_strategy(full_input, prediction)

        return LLMResponse(
            success=True,
            llm_provider=consultant.active_llm,
            prediction=prediction,
            analysis=result,
        )
    except Exception as e:
        logger.error("ì „ëµ ì œì•ˆ ì‹¤íŒ¨: %s", e, exc_info=True)
        raise HTTPException(500, detail=str(e))


@app.post("/api/v1/ask", response_model=LLMResponse)
async def ask(req: QARequest):
    """Q&A ëŒ€í™”"""
    try:
        predictor = get_predictor()
        consultant = get_consultant()

        input_data = req.prediction_input.to_dict()
        prediction = predictor.predict(input_data)
        full_input = {**input_data, **req.prediction_input.model_dump()}

        answer = consultant.ask(
            question=req.question,
            input_data=full_input,
            prediction=prediction,
            chat_history=req.chat_history,
        )

        return LLMResponse(
            success=True,
            llm_provider=consultant.active_llm,
            prediction=prediction,
            analysis=answer,
        )
    except Exception as e:
        logger.error("Q&A ì‹¤íŒ¨: %s", e, exc_info=True)
        raise HTTPException(500, detail=str(e))


@app.post("/api/v1/competitors", response_model=LLMResponse)
async def competitors(req: PredictionRequest):
    """ê²½ìŸì—…ì²´ ë¶„ì„"""
    try:
        predictor = get_predictor()
        consultant = get_consultant()

        input_data = req.to_dict()
        prediction = predictor.predict(input_data)
        full_input = {**input_data, **req.model_dump()}

        result = consultant.analyze_competitors(full_input, prediction)

        return LLMResponse(
            success=True,
            llm_provider=consultant.active_llm,
            prediction=prediction,
            analysis=result,
        )
    except Exception as e:
        logger.error("ê²½ìŸ ë¶„ì„ ì‹¤íŒ¨: %s", e, exc_info=True)
        raise HTTPException(500, detail=str(e))