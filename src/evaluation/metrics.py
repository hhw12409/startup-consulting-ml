"""
ğŸ“ src/evaluation/metrics.py
==============================
ëª¨ë¸ í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°.

[ì—­í• ] ë¶„ë¥˜(ìƒì¡´ ì˜ˆì¸¡) + íšŒê·€(ë§¤ì¶œ ì˜ˆì¸¡) ë©”íŠ¸ë¦­ì„ í•œ ë²ˆì— ê³„ì‚°í•©ë‹ˆë‹¤.
"""

import numpy as np
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, mean_absolute_error, mean_squared_error, r2_score,
)

from src.models.base import BaseModel
from src.utils.logger import get_logger

logger = get_logger(__name__)


def evaluate_model(
        model: BaseModel,
        X_test: np.ndarray,
        y_test: np.ndarray,
) -> dict[str, float]:
    """
    ëª¨ë¸ ì „ì²´ í‰ê°€.

    Args:
        model: í•™ìŠµëœ ëª¨ë¸
        X_test: í…ŒìŠ¤íŠ¸ í”¼ì²˜
        y_test: í…ŒìŠ¤íŠ¸ ë¼ë²¨ [N, 6]
               (survival_1yr, survival_3yr, revenue, profit, risk, break_even)

    Returns:
        ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ (ì˜ˆ: {"survival_1yr_accuracy": 0.85, ...})
    """
    preds = model.predict(X_test)
    metrics = {}

    # â”€â”€ ìƒì¡´ ì˜ˆì¸¡ (ë¶„ë¥˜) â”€â”€
    if "survival" in preds:
        for i, tag in enumerate(["1yr", "3yr"]):
            y_true = (y_test[:, i] > 0.5).astype(int)
            y_pred = (preds["survival"][:, i] > 0.5).astype(int)
            y_prob = preds["survival"][:, i]

            metrics[f"survival_{tag}_acc"] = accuracy_score(y_true, y_pred)
            metrics[f"survival_{tag}_f1"] = f1_score(y_true, y_pred, zero_division=0)
            try:
                metrics[f"survival_{tag}_auc"] = roc_auc_score(y_true, y_prob)
            except ValueError:
                metrics[f"survival_{tag}_auc"] = 0.0

    # â”€â”€ ë§¤ì¶œ ì˜ˆì¸¡ (íšŒê·€) â”€â”€
    if "revenue" in preds:
        metrics["revenue_mae"] = mean_absolute_error(y_test[:, 2], preds["revenue"][:, 0])
        metrics["revenue_r2"] = r2_score(y_test[:, 2], preds["revenue"][:, 0])
        metrics["profit_mae"] = mean_absolute_error(y_test[:, 3], preds["revenue"][:, 1])

    # â”€â”€ ë¦¬ìŠ¤í¬ (íšŒê·€) â”€â”€
    if "risk" in preds:
        metrics["risk_mae"] = mean_absolute_error(y_test[:, 4], preds["risk"][:, 0])

    # â”€â”€ ë¡œê¹… â”€â”€
    logger.info("â”â”â” í‰ê°€ ê²°ê³¼: %s â”â”â”", model.name)
    for k, v in sorted(metrics.items()):
        logger.info("  %-25s %.4f", k, v)

    return metrics